{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>1.2 Breast Cancer Wisconsin Data set</center></h1>\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "Jishnu Jeevan <br>\n",
    "Department of Computer Science <br>\n",
    "M.Tech Computer and Information Science <br>\n",
    "jishnujeevan@cusat.ac.in <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Assignemt Objective</center></h2>\n",
    "<p style='text-align: justify;'>\n",
    "The data set contains details of breast cancer patients.<br>\n",
    "The classification goal is to predict if the cancer is benign(B) or malignant(M), column diagnosis.<br>\n",
    "The link for the data set :<br>\n",
    "<b>https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/data</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The data set contains the following columns i.e features</h2>\n",
    "<p style='text-align: justify;'>\n",
    "Input variables:<br>\n",
    "Ten real-valued features are computed for each cell nucleus:<br>\n",
    "1. Radius (mean of distances from center to points on the perimeter)<br>\n",
    "2. Texture (standard deviation of gray-scale values) <br>\n",
    "3. Perimeter <br>\n",
    "4. Area <br>\n",
    "5. Smoothness (local variation in radius lengths) <br>\n",
    "6. Compactness (perimeter^2 / area - 1.0)<br>\n",
    "7. Concavity (severity of concave portions of the contour)<br>\n",
    "8. Concave points (number of concave portions of the contour)<br>\n",
    "9. Symmetry<br>\n",
    "10. Fractal dimension (\"coastline approximation\" - 1)<br>\n",
    "Here the above 10 variables are divided into three parts first is Mean(_mean), Stranded Error(_se) and Worst(_worst) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension), so a total of 30 input variable(columns).<br>\n",
    "\n",
    "Ouput variable <br>\n",
    "Diagnosis (M = malignant, B = benign)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading the data\n",
    "import pandas as pd\n",
    "\n",
    "# For training, testing and splitting of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For calculating accuracy, precision and recall and confusion matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Classification Algorithms \n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing and displaying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Let us look at the kind of data types we have for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "Unnamed: 32                0 non-null float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. We are going to remove the unwanted columns i.e. Unnamed: 32 as it has full of null values, and id as it provides no useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnamed: 32 column\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True) \n",
    "\n",
    "# Drop id column\n",
    "data.drop(\"id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. We are going to convert the object data type i.e. diagnosis to a binary value of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                   object\n",
       "radius_mean                float64\n",
       "texture_mean               float64\n",
       "perimeter_mean             float64\n",
       "area_mean                  float64\n",
       "smoothness_mean            float64\n",
       "compactness_mean           float64\n",
       "concavity_mean             float64\n",
       "concave points_mean        float64\n",
       "symmetry_mean              float64\n",
       "fractal_dimension_mean     float64\n",
       "radius_se                  float64\n",
       "texture_se                 float64\n",
       "perimeter_se               float64\n",
       "area_se                    float64\n",
       "smoothness_se              float64\n",
       "compactness_se             float64\n",
       "concavity_se               float64\n",
       "concave points_se          float64\n",
       "symmetry_se                float64\n",
       "fractal_dimension_se       float64\n",
       "radius_worst               float64\n",
       "texture_worst              float64\n",
       "perimeter_worst            float64\n",
       "area_worst                 float64\n",
       "smoothness_worst           float64\n",
       "compactness_worst          float64\n",
       "concavity_worst            float64\n",
       "concave points_worst       float64\n",
       "symmetry_worst             float64\n",
       "fractal_dimension_worst    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the data types of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the diagnosis column to 1 for malignant and 0 for benign\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Since there are no missing values we are not going to be dealing with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove the input and output varaibles from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
      "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n",
      "Index(['diagnosis'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Output variable is stored in data_y\n",
    "data_y = pd.DataFrame(data['diagnosis'])\n",
    "\n",
    "# Features are stored in data_x\n",
    "data_X = data.drop(['diagnosis'], axis=1)\n",
    "\n",
    "print(data_X.columns)\n",
    "print(data_y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. We are going to reduce the number of features in the dataset. Originally there was only 30 features. We are going to find out which of the features highly corelated with the output variable (\"diagnosis\") and only select the features that have high corelation with the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis               1.000000\n",
       "radius_mean             0.730029\n",
       "perimeter_mean          0.742636\n",
       "area_mean               0.708984\n",
       "compactness_mean        0.596534\n",
       "concavity_mean          0.696360\n",
       "concave points_mean     0.776614\n",
       "radius_se               0.567134\n",
       "perimeter_se            0.556141\n",
       "area_se                 0.548236\n",
       "radius_worst            0.776454\n",
       "perimeter_worst         0.782914\n",
       "area_worst              0.733825\n",
       "compactness_worst       0.590998\n",
       "concavity_worst         0.659610\n",
       "concave points_worst    0.793566\n",
       "Name: diagnosis, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlation with output variable\n",
    "\n",
    "# Find out the correlation matrix \n",
    "cor = data.corr()\n",
    "\n",
    "# Select the output variable (from the correlation matrix)\n",
    "cor_target = abs(cor['diagnosis'])\n",
    "\n",
    "#Selecting the most correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "\n",
    "# Print them\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  perimeter_mean    area_mean  compactness_mean  \\\n",
       "count   569.000000      569.000000   569.000000        569.000000   \n",
       "mean     14.127292       91.969033   654.889104          0.104341   \n",
       "std       3.524049       24.298981   351.914129          0.052813   \n",
       "min       6.981000       43.790000   143.500000          0.019380   \n",
       "25%      11.700000       75.170000   420.300000          0.064920   \n",
       "50%      13.370000       86.240000   551.100000          0.092630   \n",
       "75%      15.780000      104.100000   782.700000          0.130400   \n",
       "max      28.110000      188.500000  2501.000000          0.345400   \n",
       "\n",
       "       concavity_mean  concave points_mean   radius_se  perimeter_se  \\\n",
       "count      569.000000           569.000000  569.000000    569.000000   \n",
       "mean         0.088799             0.048919    0.405172      2.866059   \n",
       "std          0.079720             0.038803    0.277313      2.021855   \n",
       "min          0.000000             0.000000    0.111500      0.757000   \n",
       "25%          0.029560             0.020310    0.232400      1.606000   \n",
       "50%          0.061540             0.033500    0.324200      2.287000   \n",
       "75%          0.130700             0.074000    0.478900      3.357000   \n",
       "max          0.426800             0.201200    2.873000     21.980000   \n",
       "\n",
       "          area_se  radius_worst  perimeter_worst   area_worst  \\\n",
       "count  569.000000    569.000000       569.000000   569.000000   \n",
       "mean    40.337079     16.269190       107.261213   880.583128   \n",
       "std     45.491006      4.833242        33.602542   569.356993   \n",
       "min      6.802000      7.930000        50.410000   185.200000   \n",
       "25%     17.850000     13.010000        84.110000   515.300000   \n",
       "50%     24.530000     14.970000        97.660000   686.500000   \n",
       "75%     45.190000     18.790000       125.400000  1084.000000   \n",
       "max    542.200000     36.040000       251.200000  4254.000000   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \n",
       "count         569.000000       569.000000            569.000000  \n",
       "mean            0.254265         0.272188              0.114606  \n",
       "std             0.157336         0.208624              0.065732  \n",
       "min             0.027290         0.000000              0.000000  \n",
       "25%             0.147200         0.114500              0.064930  \n",
       "50%             0.211900         0.226700              0.099930  \n",
       "75%             0.339100         0.382900              0.161400  \n",
       "max             1.058000         1.252000              0.291000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List out the features you are going to use\n",
    "features = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concavity_mean','concave points_mean', \n",
    "            'radius_se','perimeter_se','area_se', \n",
    "            'radius_worst','perimeter_worst','area_worst','compactness_worst','concavity_worst','concave points_worst']\n",
    "\n",
    "data_X = data_X[features]\n",
    "data_X.describe()\n",
    "#data_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We are going to be splitting the dataset into training (70% of the dataset) and testing (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.3, random_state=2, stratify=data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. We are going to use different binary classification algorithms to do classification on the data, and we will evaluate the accuarcy of each algorithm.\n",
    "### The algorithms we are going to use are\n",
    "1. XGB Classifier\n",
    "2. Logistics Regression\n",
    "3. Random Forest Classifier\n",
    "4. Decision Tree\n",
    "5. Linear Discriminant Analysis\n",
    "6. K Nearest Neighbour\n",
    "7. Gaussian Naive Bayes \n",
    "8. AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name :  1. XGB Classifier\n",
      "Accuracy :  0.9298245614035088\n",
      "Cross validation score :  [0.94029851 0.96969697 0.96212121]\n",
      "Precision :  0.9343149221849252\n",
      "Recall :  0.915668808411215\n",
      "Confusion matrix : \n",
      " [[104   3]\n",
      " [  9  55]]\n",
      "\n",
      "\n",
      "Name :  2. Logistic Regression\n",
      "Accuracy :  0.9473684210526315\n",
      "Cross validation score :  [0.94029851 0.95454545 0.9469697 ]\n",
      "Precision :  0.9451058201058201\n",
      "Recall :  0.9422459112149533\n",
      "Confusion matrix : \n",
      " [[103   4]\n",
      " [  5  59]]\n",
      "\n",
      "\n",
      "Name :  3. Random Forest Classifier\n",
      "Accuracy :  0.935672514619883\n",
      "Cross validation score :  [0.94776119 0.95454545 0.9469697 ]\n",
      "Precision :  0.9388619854721549\n",
      "Recall :  0.923481308411215\n",
      "Confusion matrix : \n",
      " [[104   3]\n",
      " [  8  56]]\n",
      "\n",
      "\n",
      "Name :  4. Decision Tree Classifier\n",
      "Accuracy :  0.9181286549707602\n",
      "Cross validation score :  [0.92537313 0.96212121 0.93939394]\n",
      "Precision :  0.9095516847515706\n",
      "Recall :  0.9188814252336448\n",
      "Confusion matrix : \n",
      " [[98  9]\n",
      " [ 5 59]]\n",
      "\n",
      "\n",
      "Name :  5. Linear Discriminant Analysis\n",
      "Accuracy :  0.9239766081871345\n",
      "Cross validation score :  [0.93283582 0.93939394 0.92424242]\n",
      "Precision :  0.9458333333333333\n",
      "Recall :  0.8984375\n",
      "Confusion matrix : \n",
      " [[107   0]\n",
      " [ 13  51]]\n",
      "\n",
      "\n",
      "Name :  6. K Nearest Neighbour\n",
      "Accuracy :  0.935672514619883\n",
      "Cross validation score :  [0.8880597  0.9469697  0.93939394]\n",
      "Precision :  0.9353949329359166\n",
      "Recall :  0.9266209112149533\n",
      "Confusion matrix : \n",
      " [[103   4]\n",
      " [  7  57]]\n",
      "\n",
      "\n",
      "Name :  7. Gaussian Naive Bayes Classifier\n",
      "Accuracy :  0.9005847953216374\n",
      "Cross validation score :  [0.93283582 0.93939394 0.93939394]\n",
      "Precision :  0.907601880877743\n",
      "Recall :  0.8797459112149533\n",
      "Confusion matrix : \n",
      " [[103   4]\n",
      " [ 13  51]]\n",
      "\n",
      "\n",
      "Name :  8. Adaptive Boosting Classifier\n",
      "Accuracy :  0.9298245614035088\n",
      "Cross validation score :  [0.94776119 0.97727273 0.96212121]\n",
      "Precision :  0.9275673276117194\n",
      "Recall :  0.9219480140186915\n",
      "Confusion matrix : \n",
      " [[102   5]\n",
      " [  7  57]]\n"
     ]
    }
   ],
   "source": [
    "# To surpress warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a dictionary to find out the best classifier using accuracy score\n",
    "ranking = {}\n",
    "\n",
    "# We are going to be using the following classifiers and doing a comparision study\n",
    "classifiers = {\n",
    "                '1. XGB Classifier':XGBClassifier(n_estimator = 500, learning_rate = 0.5),\n",
    "                '2. Logistic Regression':LogisticRegression(),\n",
    "                '3. Random Forest Classifier': RandomForestClassifier(),\n",
    "                '4. Decision Tree Classifier':DecisionTreeClassifier(),\n",
    "                '5. Linear Discriminant Analysis':LinearDiscriminantAnalysis(),               \n",
    "                '6. K Nearest Neighbour':KNeighborsClassifier(8),                \n",
    "                '7. Gaussian Naive Bayes Classifier':GaussianNB(),\n",
    "                '8. Adaptive Boosting Classifier':AdaBoostClassifier()\n",
    "               }\n",
    "\n",
    "# Take each classifier from list\n",
    "for Name, classifier in classifiers.items():\n",
    "    # Fit the model using the training set\n",
    "    classifier.fit(X_train ,y_train)\n",
    "    \n",
    "    # Find out the predicion using test set\n",
    "    y_predicted = classifier.predict(X_test)\n",
    "    \n",
    "    # Find out the accuracy using the y test set and perdicted valur of y\n",
    "    accuracy = metrics.accuracy_score(y_test,y_predicted)\n",
    "    \n",
    "    # Cross validaion score\n",
    "    score = cross_val_score(classifier, X_train, y_train, cv=3)\n",
    "    \n",
    "    # Find out percision\n",
    "    precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "    \n",
    "    # Find out recall\n",
    "    recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "         \n",
    "    # Confusion matrix\n",
    "    confusion = confusion_matrix(y_test, y_predicted)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\")\n",
    "    print(\"Name : \", Name)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Cross validation score : \", score)\n",
    "    print(\"Precision : \", precision)\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"Confusion matrix : \\n\", confusion)\n",
    "    \n",
    "    # Add the name of classifier and accuracy score to dictionary\n",
    "    ranking[Name] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The aglorithms that perform well, accorging to there accuracy score are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Logistic Regression ; 0.9473684210526315\n",
      "3. Random Forest Classifier ; 0.935672514619883\n",
      "6. K Nearest Neighbour ; 0.935672514619883\n",
      "1. XGB Classifier ; 0.9298245614035088\n",
      "8. Adaptive Boosting Classifier ; 0.9298245614035088\n",
      "5. Linear Discriminant Analysis ; 0.9239766081871345\n",
      "4. Decision Tree Classifier ; 0.9181286549707602\n",
      "7. Gaussian Naive Bayes Classifier ; 0.9005847953216374\n"
     ]
    }
   ],
   "source": [
    "# Sort the dictionary 'ranking' accoriding to highest accuracy\n",
    "print(\"\\n\")\n",
    "ranking_sorted = sorted(ranking.items(),  reverse = True, key=lambda x: x[1]) # This returns a tuple, not a dictinary\n",
    "for k,v in ranking_sorted:\n",
    "    print(k, \";\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Same as for bank market analasis, we can also use support vector classifier (SVC) on the dataset. But due to the large running time of the algorithm, I have decided to abandon it as it takes a very long time to give the output, compared to the above algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
